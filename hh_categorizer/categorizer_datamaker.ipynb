{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shitkov/categorizer/blob/main/categorizer_datamaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jeew3d7gBFgF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://github.com/shitkov/categorizer/raw/main/data_train.zip\n",
        "!wget https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt\n",
        "!unzip /content/data_train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF1dHs5tBW_k"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/HeadHunter_train.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODsgbPawBYqS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxyOXUzOBaOL"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8uQJtY3Bb_V"
      },
      "outputs": [],
      "source": [
        "data = data.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REenCPk-F3cQ"
      },
      "outputs": [],
      "source": [
        "# onehot для кажого таргета\n",
        "for i in range(0, 9):\n",
        "    col_name = 'tag_' + str(i)\n",
        "    labels = [1 if str(i) in t.split(',') else 0 for t in list(data['target'])]\n",
        "    data[col_name] = pd.Series(labels, index=data.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtskT6y3Genu"
      },
      "outputs": [],
      "source": [
        "# длина отзыва\n",
        "data['positive_len'] = [len(str(pos)) for pos in list(data['positive'])]\n",
        "data['negative_len'] = [len(str(pos)) for pos in list(data['negative'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka0xRX29G9-R"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtv_LPGxG_1H"
      },
      "outputs": [],
      "source": [
        "with open('/content/stopwords-ru.txt') as f:\n",
        "    stopwords = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdX7oSklHKjq"
      },
      "outputs": [],
      "source": [
        "stopwords = [line.rstrip('\\n') for line in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymystem3\n",
        "\n",
        "from pymystem3 import Mystem\n",
        "mstm = Mystem()\n",
        "\n",
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem"
      ],
      "metadata": {
        "id": "MD-zKOx0-ykZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymystem3 import Mystem\n",
        "lemmatizer = Mystem()"
      ],
      "metadata": {
        "id": "pPbC-CGV-zki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V70L5sZHKmM"
      },
      "outputs": [],
      "source": [
        "# почистить + лемматизировать отзывы\n",
        "def get_clean_text(texts, stopwords, lemmatizer):\n",
        "    texts = [re.sub('[^а-яё ]', ' ', str(t).lower()) for t in texts]\n",
        "    texts = [re.sub(r\" +\", \" \", t).strip() for t in texts]\n",
        "    clean_texts = []\n",
        "    for text in texts:\n",
        "        if len(text) > 0:\n",
        "            lemmatized_text_list = [token for token in lemmatizer.lemmatize(text)[:-1] if token != ' ']\n",
        "            text = ' '.join([word for word in lemmatized_text_list if word not in stopwords])\n",
        "            # костыль для сокращения\n",
        "            text = text.replace('з п', 'зп')\n",
        "        clean_texts.append(text)\n",
        "    return clean_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnuzQ_SiHKop"
      },
      "outputs": [],
      "source": [
        "data['clean_positive'] = get_clean_text(list(data['positive']), stopwords, lemmatizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qav6ogYGI6TE"
      },
      "outputs": [],
      "source": [
        "data['clean_negative'] = get_clean_text(list(data['negative']), stopwords, lemmatizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCyZfucZHKrG"
      },
      "outputs": [],
      "source": [
        "# количество уникальных слов в отзыве\n",
        "data['unique_positive'] = [len(list(set(t.split(' ')))) for t in list(data['clean_positive'])]\n",
        "data['unique_negative'] = [len(list(set(t.split(' ')))) for t in list(data['clean_negative'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoxy6xwHOCnk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI7IrAw4KawZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_ovSdGVKazE"
      },
      "outputs": [],
      "source": [
        "# Sentiment\n",
        "model_checkpoint = 'cointegrated/rubert-tiny-sentiment-balanced'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "def get_sentiment(text, return_type='label'):\n",
        "    \"\"\" Calculate sentiment of a text. `return_type` can be 'label', 'score' or 'proba' \"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)\n",
        "        proba = torch.sigmoid(model(**inputs).logits).cpu().numpy()[0]\n",
        "    if return_type == 'label':\n",
        "        return model.config.id2label[proba.argmax()]\n",
        "    elif return_type == 'score':\n",
        "        return proba.dot([-1, 0, 1])\n",
        "    return proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgT-yBivOo14"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUlDR9emKa1V"
      },
      "outputs": [],
      "source": [
        "data['sentiment_positive_label'] = [get_sentiment(text, 'label') for text in tqdm(list(data['positive']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHXm6KPFOMWC"
      },
      "outputs": [],
      "source": [
        "data['sentiment_negative_label'] = [get_sentiment(text, 'label') for text in tqdm(list(data['negative']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01sP0quqQTtw"
      },
      "outputs": [],
      "source": [
        "data['sentiment_positive_score'] = [get_sentiment(text, 'score') for text in tqdm(list(data['positive']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P731srdqQTwm"
      },
      "outputs": [],
      "source": [
        "data['sentiment_negative_score'] = [get_sentiment(text, 'score') for text in tqdm(list(data['negative']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANDodbKaOMYs"
      },
      "outputs": [],
      "source": [
        "# Emotion detection\n",
        "# LABELS = ['no_emotion', 'joy', 'sadness', 'surprise', 'fear', 'anger']\n",
        "model_checkpoint = 'cointegrated/rubert-tiny2-cedr-emotion-detection'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "def get_emotion(text, return_type='label'):\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)\n",
        "        proba = torch.sigmoid(model(**inputs).logits).cpu().numpy()[0]\n",
        "    if return_type == 'label':\n",
        "        return model.config.id2label[proba.argmax()]\n",
        "    elif return_type == 'score':\n",
        "        return proba.dot([0,1,-1,0,-1,-1])\n",
        "    return proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgG_aRyNQzJb"
      },
      "outputs": [],
      "source": [
        "data['emotion_positive_label'] = [get_emotion(text, 'label') for text in tqdm(list(data['positive']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qp2ogmxQzQ4"
      },
      "outputs": [],
      "source": [
        "data['emotion_negative_label'] = [get_emotion(text, 'label') for text in tqdm(list(data['negative']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNLxY0_0OMdY"
      },
      "outputs": [],
      "source": [
        "data['emotion_positive_score'] = [get_emotion(text, 'score') for text in tqdm(list(data['positive']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-WA0-IRQsYT"
      },
      "outputs": [],
      "source": [
        "data['emotion_negative_score'] = [get_emotion(text, 'score') for text in tqdm(list(data['negative']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJKc5n0iRHdo"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = 'cointegrated/rubert-tiny-toxicity'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "def text2toxicity(text, return_type = 'label'):\n",
        "    \"\"\" Calculate toxicity of a text (if aggregate=True) or a vector of toxicity aspects (if aggregate=False)\"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)\n",
        "        proba = torch.sigmoid(model(**inputs).logits).cpu().numpy()\n",
        "    if isinstance(text, str):\n",
        "        proba = proba[0]\n",
        "    if return_type == 'label':\n",
        "        return model.config.id2label[proba.argmax()]\n",
        "    elif return_type == 'score':\n",
        "        return 1 - proba.T[0] * (1 - proba.T[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_1dLSCfRJwk"
      },
      "outputs": [],
      "source": [
        "data['toxic_positive_label'] = [text2toxicity(text, 'label') for text in (list(data['positive']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrmHKa6ORJwm"
      },
      "outputs": [],
      "source": [
        "data['toxic_negative_label'] = [text2toxicity(text, 'label') for text in (list(data['negative']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiN81KSNRJwm"
      },
      "outputs": [],
      "source": [
        "data['toxic_positive_score'] = [text2toxicity(text, 'score') for text in (list(data['positive']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDiwd97DRJwn"
      },
      "outputs": [],
      "source": [
        "data['toxic_negative_score'] = [text2toxicity(text, 'score') for text in (list(data['negative']))]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Для сохранения пропорций разбить отдельно для каждой метки и склеить\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "targets = ['tag_0', 'tag_1', 'tag_2', 'tag_3', 'tag_4', 'tag_5', 'tag_6', 'tag_7', 'tag_8']\n",
        "df_list_train = []\n",
        "df_list_valid = []\n",
        "df_list_test = []\n",
        "for target in targets:\n",
        "    df = data[data[target] == 1]\n",
        "    df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)\n",
        "    df_valid, df_test = train_test_split(df_test, test_size=0.5, random_state=42)\n",
        "    df_list_train.append(df_train)\n",
        "    df_list_valid.append(df_valid)\n",
        "    df_list_test.append(df_test)"
      ],
      "metadata": {
        "id": "45Khk4OQ5awN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.concat(df_list_train).sort_values(by='review_id').reset_index(drop=True)\n",
        "data_valid = pd.concat(df_list_valid).sort_values(by='review_id').reset_index(drop=True)\n",
        "data_test = pd.concat(df_list_test).sort_values(by='review_id').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "W9yYp1sg76rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.to_csv('data_train.csv', index = False)\n",
        "data_valid.to_csv('data_valid.csv', index = False)\n",
        "data_test.to_csv('data_test.csv', index = False)"
      ],
      "metadata": {
        "id": "MHbcr4R_8vxU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "categorizer_datamaker.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmuNDp+/b6yVVj/sn9m9h2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}